
@article{tutt_fda_2017,
	title = {An Fda for Algorithms},
	volume = {69},
	issn = {0001-8368},
	url = {https://www.jstor.org/stable/44648608},
	abstract = {The rise of increasingly complex algorithms calls for critical thought about how best to prevent, deter, and compensate for the harms that they cause. This Article argues that the criminal law and tort regulatory systems will prove no matchfor the difficult regulatory puzzles algorithms pose. Algorithmic regulation will require federal uniformity, expert judgment, political independence, and pre-market review to prevent—without stifling innovation—the introduction of unacceptably dangerous algorithms into the market. This Article proposes that certain classes of new algorithms should not be permitted to be distributed or sold without approval from a government agency designed along the lines of the {FDA}. This "{FDA} for Algorithms" would approve certain complex and dangerous algorithms when it could be shown that they would be safe and effective for their intended use and that satisfactory measures would be taken to prevent their harmful misuse. Lastly, this Article proposes that the agency should serve as a centralized expert regulator that develops guidance, standards, and expertise in partnership with industry to strike a balance between innovation and safety.},
	pages = {83--123},
	number = {1},
	journaltitle = {Administrative Law Review},
	author = {Tutt, Andrew},
	urldate = {2025-11-28},
	date = {2017},
	note = {Publisher: American Bar Association},
	keywords = {⛔ No {DOI} found},
}

@article{citron_scored_2014,
	title = {The Scored Society: Due Process for Automated Predictions},
	volume = {89},
	rights = {Copyright Washington Law Review Association Mar 2014},
	issn = {00430617},
	url = {https://www.proquest.com/docview/1518882607/abstract/9F5B50ED157E4688PQ/1},
	shorttitle = {The Scored Society},
	abstract = {Big Data is increasingly mined to rank and rate individuals. Predictive algorithms assess whether they are good credit risks, desirable employees, reliable tenants, valuable customers -- or deadbeats, shirkers, menaces, and "wastes of time." Crucial opportunities are on the line, including the ability to obtain loans, work, housing, and insurance. Though automated scoring is pervasive and consequential, it is also opaque and lacking oversight. In one area where regulation does prevail -- credit -- the law focuses on credit history, not the derivation of scores from data. Procedural regularity is essential for those stigmatized by "artificially intelligent" scoring systems. The American due process tradition should inform basic safeguards. Regulators should be able to test scoring systems to ensure their fairness and accuracy. Individuals should be granted meaningful opportunities to challenge adverse decisions based on scores miscategorizing them. Without such protections in place, systems could launder biased and arbitrary data into powerfully stigmatizing scores.},
	pages = {1--33},
	number = {1},
	journaltitle = {Washington Law Review},
	author = {Citron, Danielle Keats and Pasquale, Frank},
	urldate = {2025-11-28},
	date = {2014-03},
	note = {Num Pages: 33
Place: Seattle, United States
Publisher: Washington Law Review Association},
	keywords = {⛔ No {DOI} found, Stigma, Federal regulation, Artificial intelligence, Robots, Algorithms, Big Data, Automation, Awards \& honors, Credit scoring, Due process of law, Generic products, Learning, Lessors, Secondary school students, Software},
}

@article{coglianese_transparency_2019,
	title = {Transparency and Algorithmic Governance},
	volume = {71},
	issn = {0001-8368},
	url = {https://www.jstor.org/stable/27170531},
	abstract = {Machine-learning algorithms are improving and automating important functions in medicine, transportation, and business. Government officials have also started to take notice of the accuracy and speed that such algorithms provide, increasingly relying on them to aid with consequential public-sector functions, including tax administration, regulatory oversight, and benefits administration. Despite machine-learning algorithms’ superior predictive power over conventional analytic tools, algorithmic forecasts are difficult to understand and explain. Machine learning’s “black box” nature has thus raised concern: Can algorithmic governance be squared with legal principles of governmental transparency? We analyze this question and conclude that machine-learning algorithms’ relative inscrutability does not pose a legal barrier to their responsible use by governmental authorities. We distinguish between principles of “fishbowl transparency” and “reasoned transparency,” explaining how both are implicated by algorithmic governance but also showing that neither conception compels anything close to total transparency. Although machine learning’s black-box features distinctively implicate notions of reasoned transparency, legal demands for reason-giving can be satisfied by explaining an algorithm’s purpose, design, and basic functioning. Furthermore, new technical advances will only make machine-learning algorithms increasingly more explainable. Algorithmic governance can meet both legal and public demands for transparency while still enhancing accuracy, efficiency, and even potentially legitimacy in government.},
	pages = {1--56},
	number = {1},
	journaltitle = {Administrative Law Review},
	author = {Coglianese, Cary and Lehr, David},
	urldate = {2025-11-28},
	date = {2019},
	note = {Publisher: American Bar Association},
	keywords = {⛔ No {DOI} found},
}

@article{maronese_quantum_2023,
	title = {The Quantum Amplitude Estimation Algorithms on Near-Term Devices: A Practical Guide},
	volume = {6},
	issn = {2624-960X},
	url = {https://www.mdpi.com/2624-960X/6/1/1},
	doi = {10.3390/quantum6010001},
	shorttitle = {The Quantum Amplitude Estimation Algorithms on Near-Term Devices},
	abstract = {The Quantum Amplitude Estimation ({QAE}) algorithm is a major quantum algorithm designed to achieve a quadratic speed-up. Until fault-tolerant quantum computing is achieved, being competitive over classical Monte Carlo ({MC}) remains elusive. Alternative methods have been developed so as to require fewer resources while maintaining an advantageous theoretical scaling. We compared the standard {QAE} algorithm with two Noisy Intermediate-Scale Quantum ({NISQ})-friendly versions of {QAE} on a numerical integration task, with the Monte Carlo technique of Metropolis–Hastings as a classical benchmark. The algorithms were evaluated in terms of the estimation error as a function of the number of samples, computational time, and length of the quantum circuits required by the solutions, respectively. The effectiveness of the two {QAE} alternatives was tested on an 11-qubit trapped-ion quantum computer in order to verify which solution can first provide a speed-up in the integral estimation problems. We concluded that an alternative approach is preferable with respect to employing the phase estimation routine. Indeed, the Maximum Likelihood estimation guaranteed the best trade-off between the length of the quantum circuits and the precision in the integral estimation, as well as greater resistance to noise.},
	pages = {1--13},
	number = {1},
	journaltitle = {Quantum Reports},
	author = {Maronese, Marco and Incudini, Massimiliano and Asproni, Luca and Prati, Enrico},
	urldate = {2025-11-27},
	date = {2023-12-01},
	langid = {english},
	note = {Publisher: {MDPI} {AG}},
}

@misc{bornman_introduction_2023,
	title = {An introduction to financial option pricing on a qudit-based quantum computer},
	url = {http://arxiv.org/abs/2311.05537},
	doi = {10.48550/arXiv.2311.05537},
	abstract = {The financial sector is anticipated to be one of the first industries to benefit from the increased computational power of quantum computers, in areas such as portfolio optimisation and risk management to financial derivative pricing. Financial mathematics, and derivative pricing in particular, are not areas quantum physicists are traditionally trained in despite the fact that they often have the raw technical skills needed to understand such topics. On the other hand, most quantum algorithms have largely focused on qubits, which are comprised of two discrete states, as the information carriers. However, discrete higher-dimensional qudits, in addition to possibly possessing increased noise robustness and allowing for novel error correction protocols in certain hardware implementations, also have logarithmically greater information storage and processing capacity. In the current {NISQ} era of quantum computing, a wide array of hardware paradigms are still being studied and any potential advantage a platform offers is worth exploring. Here we introduce the basic concepts behind financial derivatives for the unfamiliar enthusiast as well as outline in great detail the quantum algorithm routines needed to price a European option, the simplest derivative. This is done within the context of a quantum computer comprised of qudits and employing the natural higher-dimensional analogue of a qubit-based pricing algorithm with its various subroutines. From these pieces, one should relatively easily be able to tailor the scheme to more complex, realistic financial derivatives. Finally, the entire stack is numerically simulated with the results demonstrating how the qudit-based scheme's payoff quickly approaches that of both a similarly-resourced classical computer as well as the true payoff, within error, for a modest increase in qudit dimension.},
	number = {{arXiv}:2311.05537},
	publisher = {{arXiv}},
	author = {Bornman, Nicholas},
	urldate = {2025-11-27},
	date = {2023-11-09},
	eprinttype = {arxiv},
	eprint = {2311.05537 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{grinko_iterative_2021,
	title = {Iterative Quantum Amplitude Estimation},
	volume = {7},
	issn = {2056-6387},
	url = {http://arxiv.org/abs/1912.05559},
	doi = {10.1038/s41534-021-00379-1},
	abstract = {We introduce a new variant of Quantum Amplitude Estimation ({QAE}), called Iterative {QAE} ({IQAE}), which does not rely on Quantum Phase Estimation ({QPE}) but is only based on Grover's Algorithm, which reduces the required number of qubits and gates. We provide a rigorous analysis of {IQAE} and prove that it achieves a quadratic speedup up to a double-logarithmic factor compared to classical Monte Carlo simulation. Furthermore, we show with an empirical study that our algorithm outperforms other known {QAE} variants without {QPE}, some even by orders of magnitude, i.e., our algorithm requires significantly fewer samples to achieve the same estimation accuracy and confidence level.},
	pages = {52},
	number = {1},
	journaltitle = {npj Quantum Information},
	shortjournal = {npj Quantum Inf},
	author = {Grinko, Dmitry and Gacon, Julien and Zoufal, Christa and Woerner, Stefan},
	urldate = {2025-11-27},
	date = {2021-03-19},
	eprinttype = {arxiv},
	eprint = {1912.05559 [quant-ph]},
	keywords = {Quantum Physics},
}

@misc{fernandez_energy_2025,
	title = {Energy Considerations of Large Language Model Inference and Efficiency Optimizations},
	url = {http://arxiv.org/abs/2504.17674},
	doi = {10.48550/arXiv.2504.17674},
	abstract = {As large language models ({LLMs}) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing ({NLP}) and generative Artificial Intelligence ({AI}) workloads, including conversational {AI} and code generation. We introduce a modeling approach that approximates real-world {LLM} workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, {GPU} architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on {FLOPs} or theoretical {GPU} utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73\% from unoptimized baselines. These insights provide a foundation for sustainable {LLM} deployment and inform energy-efficient design strategies for future {AI} infrastructure.},
	number = {{arXiv}:2504.17674},
	publisher = {{arXiv}},
	author = {Fernandez, Jared and Na, Clara and Tiwari, Vashisth and Bisk, Yonatan and Luccioni, Sasha and Strubell, Emma},
	urldate = {2025-11-27},
	date = {2025-04-24},
	eprinttype = {arxiv},
	eprint = {2504.17674 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{strubell_energy_2019,
	title = {Energy and Policy Considerations for Deep Learning in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	doi = {10.48550/arXiv.1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many {NLP} tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of {NLP} researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for {NLP}. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in {NLP} research and practice.},
	number = {{arXiv}:1906.02243},
	publisher = {{arXiv}},
	author = {Strubell, Emma and Ganesh, Ananya and {McCallum}, Andrew},
	urldate = {2025-11-27},
	date = {2019-06-05},
	eprinttype = {arxiv},
	eprint = {1906.02243 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{stamatopoulos_quantum_2024,
	title = {Quantum Risk Analysis of Financial Derivatives},
	url = {http://arxiv.org/abs/2404.10088},
	doi = {10.48550/arXiv.2404.10088},
	abstract = {We introduce two quantum algorithms to compute the Value at Risk ({VaR}) and Conditional Value at Risk ({CVaR}) of financial derivatives using quantum computers: the first by applying existing ideas from quantum risk analysis to derivative pricing, and the second based on a novel approach using Quantum Signal Processing ({QSP}). Previous work in the literature has shown that quantum advantage is possible in the context of individual derivative pricing and that advantage can be leveraged in a straightforward manner in the estimation of the {VaR} and {CVaR}. The algorithms we introduce in this work aim to provide an additional advantage by encoding the derivative price over multiple market scenarios in superposition and computing the desired values by applying appropriate transformations to the quantum system. We perform complexity and error analysis of both algorithms, and show that while the two algorithms have the same asymptotic scaling the {QSP}-based approach requires significantly fewer quantum resources for the same target accuracy. Additionally, by numerically simulating both quantum and classical {VaR} algorithms, we demonstrate that the quantum algorithm can extract additional advantage from a quantum computer compared to individual derivative pricing. Specifically, we show that under certain conditions {VaR} estimation can lower the latest published estimates of the logical clock rate required for quantum advantage in derivative pricing by up to \${\textbackslash}sim 30\$x. In light of these results, we are encouraged that our formulation of derivative pricing in the {QSP} framework may be further leveraged for quantum advantage in other relevant financial applications, and that quantum computers could be harnessed more efficiently by considering problems in the financial sector at a higher level.},
	number = {{arXiv}:2404.10088},
	publisher = {{arXiv}},
	author = {Stamatopoulos, Nikitas and Clader, B. David and Woerner, Stefan and Zeng, William J.},
	urldate = {2025-11-27},
	date = {2024-04-15},
	eprinttype = {arxiv},
	eprint = {2404.10088 [quant-ph]},
	note = {version: 1},
	keywords = {Quantitative Finance - Computational Finance, Quantum Physics},
}

@misc{hellstern_quantum_2025,
	title = {Quantum Leap in Finance: Economic Advantages, Security, and Post-Quantum Readiness},
	url = {http://arxiv.org/abs/2508.21548},
	doi = {10.48550/arXiv.2508.21548},
	shorttitle = {Quantum Leap in Finance},
	abstract = {This paper provides an in-depth review of the evolving role of quantum computing in the financial sector, emphasizing both its computational potential and cybersecurity implications. Distinguishing itself from existing surveys, this work integrates classical quantum computing applications - such as portfolio optimization, risk analysis, derivative pricing, and Monte Carlo simulations with a thorough examination of blockchain technologies and post-quantum cryptography ({PQC}), which are crucial for maintaining secure financial operations in the emerging quantum era. We propose a structured four-step framework to assess the feasibility and expected benefits of implementing quantum solutions in finance, considering factors such as computational scalability, error tolerance, data complexity, and practical implementability. This framework is applied to a series of representative financial scenarios to identify domains where quantum approaches can surpass classical techniques. Furthermore, the paper explores the vulnerabilities quantum computing introduces to digital finance-related applications and blockchain security, including risks to digital signatures, hash functions, and randomness generation, and discusses mitigation strategies through {PQC} and quantum-resilient alternatives of classical digital finance tools and blockchain architectures. By addressing both quantum blockchain, quantum key distribution ({QKD}) as well as quantum communication networks, his review presents a more holistic perspective than prior studies, offering actionable insights for researchers, financial practitioners, and policymakers navigating the intersection of quantum computing, blockchain, and secure financial systems.},
	number = {{arXiv}:2508.21548},
	publisher = {{arXiv}},
	author = {Hellstern, Gerhard and Yeniaras, Esra},
	urldate = {2025-11-27},
	date = {2025-08-29},
	eprinttype = {arxiv},
	eprint = {2508.21548 [quant-ph]},
	note = {version: 1},
	keywords = {Quantum Physics},
}

@misc{danielsson_artificial_2025,
	title = {Artificial intelligence and financial crises},
	url = {http://arxiv.org/abs/2407.17048},
	doi = {10.48550/arXiv.2407.17048},
	abstract = {The rapid adoption of artificial intelligence ({AI}) poses new and poorly understood threats to financial stability. We use a game-theoretic model to analyse the stability impact of {AI}, finding it amplifies existing financial system vulnerabilities - leverage, liquidity stress and opacity - through superior information processing, common data, speed and strategic complementarities. The consequence is crises become faster and more severe, where the likelihood of a crisis is directly affected by how effectively the authorities engage with {AI}. In response, we propose the authorities develop their own {AI} systems and expertise, set up direct {AI}-to-{AI} communication, implement automated crisis facilities and monitor {AI} use.},
	number = {{arXiv}:2407.17048},
	publisher = {{arXiv}},
	author = {Danielsson, Jon and Uthemann, Andreas},
	urldate = {2025-11-27},
	date = {2025-05-31},
	eprinttype = {arxiv},
	eprint = {2407.17048 [econ]},
	note = {version: 2},
	keywords = {Economics - General Economics},
}

@article{pratt_risk_1964,
	title = {Risk Aversion in the Small and in the Large},
	volume = {32},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/1913738},
	doi = {10.2307/1913738},
	abstract = {This paper concerns utility functions for money. A measure of risk aversion in the small, the risk premium or insurance premium for an arbitrary risk, and a natural concept of decreasing risk aversion are discussed and related to one another. Risks are also considered as a proportion of total assets.},
	pages = {122--136},
	number = {1},
	journaltitle = {Econometrica},
	author = {Pratt, John W.},
	urldate = {2025-12-12},
	date = {1964},
	note = {Publisher: [Wiley, Econometric Society]},
}

@article{zimmerman_nefarious_2025,
	title = {Nefarious Algorithms: Rent-Fixing via Algorithmic Collusion and the Role of Intentionality in the Pursuit of Class Monopoly Rent},
	volume = {9},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2413-8851},
	url = {https://www.mdpi.com/2413-8851/9/8/315},
	doi = {10.3390/urbansci9080315},
	shorttitle = {Nefarious Algorithms},
	abstract = {Housing unaffordability and widening socio-spatial polarization continue to pervade {US} cities today. Driving this phenomenon, in part, is the increasing investment of rental housing stock by corporate landowners who rely on firms like {RealPage}, Inc. to employ advanced algorithms that determine the highest possible rent to charge tenants. {RealPage} is currently being sued for violating {US} antitrust law. This study critically examines the evidence against and in defense of {RealPage} to identify the firm’s practices as a technologically advanced strategy of pursuing class monopoly rent ({CMR}). In the process, the study brings scholarship on platform real estate into closer dialogue with land rent theory and criticism of existing antitrust law in the {US} to establish a more nuanced understanding of intentionality. We argue that the treatment of intentionality by the existing legal framework is limited in terms of recognizing the myriad ways in which {CMR} is realized in the rental housing sector, especially in contexts where {CMR} is realized without entailing explicit collusive intent among the market players. Our analysis also reveals that {RealPage}’s algorithmically enabled pursuit of {CMR} potentially widens the scale of impact across submarket boundaries that might not have otherwise been possible, exacerbating existing and entrenched patterns of socio-economic segregation and socio-spatial inequality. We conclude by discussing the implications of the study’s findings for policy with an emphasis on the kinds of policies intended (or designed) to suppress the pursuit of {CMR} in the first place.},
	pages = {315},
	number = {8},
	journaltitle = {Urban Science},
	author = {Zimmerman, Allison J. and Anderson, Matthew B.},
	urldate = {2025-12-08},
	date = {2025-08},
	langid = {english},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {antitrust, capitalism, housing, inequality, justice, scale},
}

@misc{baldacci_mean-field_2022,
	title = {A mean-field game of market-making against strategic traders},
	url = {http://arxiv.org/abs/2203.13053},
	doi = {10.48550/arXiv.2203.13053},
	abstract = {We design a market-making model à la Avellaneda-Stoikov in which the market-takers act strategically, in the sense that they design their trading strategy based on an exogenous trading signal. The market-maker chooses her quotes based on the average market-takers' behaviour, modelled through a mean-field interaction. We derive, up to the resolution of a coupled {HJB}--Fokker--Planck system, the optimal controls of the market-maker and the representative market-taker. This approach is flexible enough to incorporate different behaviours for the market-takers and takes into account the impact of their strategies on the price process.},
	number = {{arXiv}:2203.13053},
	publisher = {{arXiv}},
	author = {Baldacci, Bastien and Bergault, Philippe and Possamaï, Dylan},
	urldate = {2025-12-07},
	date = {2022-03-24},
	eprinttype = {arxiv},
	eprint = {2203.13053 [q-fin]},
	keywords = {Mathematics - Optimization and Control, Quantitative Finance - Mathematical Finance},
}

@article{loi_transparency_2021,
	title = {Transparency as design publicity: explaining and justifying inscrutable algorithms},
	volume = {23},
	issn = {1572-8439},
	url = {https://doi.org/10.1007/s10676-020-09564-w},
	doi = {10.1007/s10676-020-09564-w},
	shorttitle = {Transparency as design publicity},
	abstract = {In this paper we argue that transparency of machine learning algorithms, just as explanation, can be defined at different levels of abstraction. We criticize recent attempts to identify the explanation of black box algorithms with making their decisions (post-hoc) interpretable, focusing our discussion on counterfactual explanations. These approaches to explanation simplify the real nature of the black boxes and risk misleading the public about the normative features of a model. We propose a new form of algorithmic transparency, that consists in explaining algorithms as an intentional product, that serves a particular goal, or multiple goals (Daniel Dennet’s design stance) in a given domain of applicability, and that provides a measure of the extent to which such a goal is achieved, and evidence about the way that measure has been reached. We call such idea of algorithmic transparency “design publicity.” We argue that design publicity can be more easily linked with the justification of the use and of the design of the algorithm, and of each individual decision following from it. In comparison to post-hoc explanations of individual algorithmic decisions, design publicity meets a different demand (the demand for impersonal justification) of the explainee. Finally, we argue that when models that pursue justifiable goals (which may include fairness as avoidance of bias towards specific groups) to a justifiable degree are used consistently, the resulting decisions are all justified even if some of them are (unavoidably) based on incorrect predictions. For this argument, we rely on John Rawls’s idea of procedural justice applied to algorithms conceived as institutions.},
	pages = {253--263},
	number = {3},
	journaltitle = {Ethics and Information Technology},
	shortjournal = {Ethics Inf Technol},
	author = {Loi, Michele and Ferrario, Andrea and Viganò, Eleonora},
	urldate = {2025-11-29},
	date = {2021-09-01},
	langid = {english},
	keywords = {Transparency, Cognitive science, Computing methodologies {\textasciitilde} Artificial intelligence, Concepts and models, Explanations, Human-centered computing {\textasciitilde} {HCI} theory, Justifications, Machine learning, Philosophy of science},
}

@article{heike_transparency_2019,
	title = {Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns},
	volume = {6},
	rights = {© The Author(s) 2019. This work is licensed under the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/ (the “License”).  Notwithstanding the {ProQuest} Terms and Conditions, you may use this content in accordance with the terms of the License.},
	url = {https://www.proquest.com/docview/2313727857/879FF33CD8C548DEPQ/5?sourcetype=Scholarly%20Journals},
	doi = {10.1177/2053951719860542},
	shorttitle = {Transparency you can trust},
	abstract = {Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking.},
	number = {1},
	journaltitle = {Big Data \& Society},
	author = {Heike, Felzmann and Villaronga, Eduard Fosch and Lutz, Christoph and Aurelia, Tamò-Larrieux},
	urldate = {2025-11-29},
	date = {2019-01},
	note = {Place: London, United Kingdom
Publisher: Sage Publications Ltd.},
	keywords = {ethics, Protection, Decision making, Transparency, Artificial intelligence, automated decision-making, Communication, Context, Credibility, Data integrity, Data processing, Ethical standards, Ethics, general data protection regulation, General Data Protection Regulation, {HRI}, Human engineering, Human-computer interaction, human– computer interaction, Information dissemination, Information sharing, Information technology, Legal norms, Norms, Performativity, Policy making, Regulation, Robots, Telecommunications, transparency, Trustworthiness},
}

@article{de_laat_algorithmic_2018,
	title = {Algorithmic Decision-Making Based on Machine Learning from Big Data: Can Transparency Restore Accountability?},
	volume = {31},
	rights = {Philosophy \& Technology is a copyright of Springer, (2017). All Rights Reserved., © 2017. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the {ProQuest} Terms and Conditions, you may use this content in accordance with the terms of the License.},
	issn = {22105433},
	url = {https://www.proquest.com/docview/2163622528/abstract/EE287A6ABE35496EPQ/1},
	doi = {10.1007/s13347-017-0293-z},
	shorttitle = {Algorithmic Decision-Making Based on Machine Learning from Big Data},
	abstract = {Decision-making assisted by algorithms developed by machine learning is increasingly determining our lives. Unfortunately, full opacity about the process is the norm. Would transparency contribute to restoring accountability for such systems as is often maintained? Several objections to full transparency are examined: the loss of privacy when datasets become public, the perverse effects of disclosure of the very algorithms themselves (“gaming the system” in particular), the potential loss of companies’ competitive edge, and the limited gains in answerability to be expected since sophisticated algorithms usually are inherently opaque. It is concluded that, at least presently, full transparency for oversight bodies alone is the only feasible option; extending it to the public at large is normally not advisable. Moreover, it is argued that algorithmic decisions preferably should become more understandable; to that effect, the models of machine learning to be employed should either be interpreted ex post or be interpretable by design ex ante.},
	pages = {525},
	number = {4},
	journaltitle = {Philosophy \& Technology},
	author = {de Laat, Paul B.},
	urldate = {2025-11-29},
	date = {2018-12},
	note = {Num Pages: 525
Place: Dordrecht, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Decision making, Transparency, Machine learning, Artificial intelligence, Accountability, Algorithms, Big Data, Privacy},
}

@article{zerilli_transparency_2019,
	title = {Transparency in Algorithmic and Human Decision-Making: Is There a Double Standard?},
	volume = {32},
	rights = {Philosophy \& Technology is a copyright of Springer, (2018). All Rights Reserved.},
	issn = {22105433},
	url = {https://www.proquest.com/docview/2099743011/abstract/EE1B3EFCD1F0440APQ/1},
	doi = {10.1007/s13347-018-0330-6},
	shorttitle = {Transparency in Algorithmic and Human Decision-Making},
	abstract = {We are sceptical of concerns over the opacity of algorithmic decision tools. While transparency and explainability are certainly important desiderata in algorithmic governance, we worry that automated decision-making is being held to an unrealistically high standard, possibly owing to an unrealistically high estimate of the degree of transparency attainable from human decision-makers. In this paper, we review evidence demonstrating that much human decision-making is fraught with transparency problems, show in what respects {AI} fares little worse or better and argue that at least some regulatory proposals for explainable {AI} could end up setting the bar higher than is necessary or indeed helpful. The demands of practical reason require the justification of action to be pitched at the level of practical reason. Decision tools that support or supplant practical reasoning should not be expected to aim higher than this. We cast this desideratum in terms of Daniel Dennett’s theory of the “intentional stance” and argue that since the justification of action for human purposes takes the form of intentional stance explanation, the justification of algorithmic decisions should take the same form. In practice, this means that the sorts of explanations for algorithmic decisions that are analogous to intentional stance explanations should be preferred over ones that aim at the architectural innards of a decision tool.},
	pages = {661--683},
	number = {4},
	journaltitle = {Philosophy \& Technology},
	author = {Zerilli, John and Knott, Alistair and Maclaurin, James and Gavaghan, Colin},
	urldate = {2025-11-29},
	date = {2019-12},
	note = {Num Pages: 661-683
Place: Dordrecht, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Decision making, Transparency, Algorithms, Decision makers, Explanation, Governance, Humans, Justification},
}

@article{lepri_fair_2018,
	title = {Fair, Transparent, and Accountable Algorithmic Decision-making Processes},
	volume = {31},
	rights = {Philosophy \& Technology is a copyright of Springer, (2017). All Rights Reserved.},
	issn = {22105433},
	url = {https://www.proquest.com/docview/2163622619/abstract/5127465099A04074PQ/1},
	doi = {10.1007/s13347-017-0279-x},
	abstract = {The combination of increased availability of large amounts of fine-grained human behavioral data and advances in machine learning is presiding over a growing reliance on algorithms to address complex societal problems. Algorithmic decision-making processes might lead to more objective and thus potentially fairer decisions than those made by humans who may be influenced by greed, prejudice, fatigue, or hunger. However, algorithmic decision-making has been criticized for its potential to enhance discrimination, information and power asymmetry, and opacity. In this paper, we provide an overview of available technical solutions to enhance fairness, accountability, and transparency in algorithmic decision-making. We also highlight the criticality and urgency to engage multi-disciplinary teams of researchers, practitioners, policy-makers, and citizens to co-develop, deploy, and evaluate in the real-world algorithmic decision-making processes designed to maximize fairness and transparency. In doing so, we describe the Open Algortihms ({OPAL}) project as a step towards realizing the vision of a world where data and algorithms are used as lenses and levers in support of democracy and development.},
	pages = {611},
	number = {4},
	journaltitle = {Philosophy \& Technology},
	author = {Lepri, Bruno and Oliver, Nuria and Letouzé, Emmanuel and Pentland, Alex and Vinck, Patrick},
	urldate = {2025-11-28},
	date = {2018-12},
	note = {Num Pages: 611
Place: Dordrecht, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Prejudice, Decision making, Transparency, Democracy, Machine learning, Policy making, Accountability, Algorithms, Discrimination, Fairness, Greed, Hunger, Multidisciplinary teams, Political development, Urgency},
}

@misc{kroll_accountable_2016,
	location = {Rochester, {NY}},
	title = {Accountable Algorithms},
	url = {https://papers.ssrn.com/abstract=2765268},
	abstract = {Many important decisions historically made by people are now made by computers. Algorithms count votes, approve loan and credit card applications, target citizens or neighborhoods for police scrutiny, select taxpayers for an {IRS} audit, and grant or deny immigration visas.},
	number = {2765268},
	publisher = {Social Science Research Network},
	author = {Kroll, Joshua A. and Huey, Joanna and Barocas, Solon and Felten, Edward W. and Reidenberg, Joel R. and Robinson, David G. and Yu, Harlan},
	urldate = {2025-11-28},
	date = {2016-03-02},
	langid = {english},
	keywords = {algorithms, governance, code, accountability, bias, big data, computational methods, computer science, cyber, discrimination, internet, law, technology},
}

@inproceedings{raji_closing_2020,
	location = {New York, {NY}, {USA}},
	title = {Closing the {AI} accountability gap: defining an end-to-end framework for internal algorithmic auditing},
	isbn = {978-1-4503-6936-7},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372873},
	doi = {10.1145/3351095.3372873},
	series = {{FAT}* '20},
	shorttitle = {Closing the {AI} accountability gap},
	abstract = {Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.},
	pages = {33--44},
	booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N. and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
	urldate = {2025-11-28},
	date = {2020-01-27},
}

@article{wu_large_2019,
	title = {Large teams develop and small teams disrupt science and technology},
	volume = {566},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-0941-9},
	doi = {10.1038/s41586-019-0941-9},
	abstract = {One of the most universal trends in science and technology today is the growth of large teams in all areas, as solitary researchers and small teams diminish in prevalence1–3. Increases in team size have been attributed to the specialization of scientific activities3, improvements in communication technology4,5, or the complexity of modern problems that require interdisciplinary solutions6–8. This shift in team size raises the question of whether and how the character of the science and technology produced by large teams differs from that of small teams. Here we analyse more than 65 million papers, patents and software products that span the period 1954–2014, and demonstrate that across this period smaller teams have tended to disrupt science and technology with new ideas and opportunities, whereas larger teams have tended to develop existing ones. Work from larger teams builds on more-recent and popular developments, and attention to their work comes immediately. By contrast, contributions by smaller teams search more deeply into the past, are viewed as disruptive to science and technology and succeed further into the future—if at all. Observed differences between small and large teams are magnified for higher-impact work, with small teams known for disruptive work and large teams for developing work. Differences in topic and research design account for a small part of the relationship between team size and disruption; most of the effect occurs at the level of the individual, as people move between smaller and larger teams. These results demonstrate that both small and large teams are essential to a flourishing ecology of science and technology, and suggest that, to achieve this, science policies should aim to support a diversity of team sizes.},
	pages = {378--382},
	number = {7744},
	journaltitle = {Nature},
	author = {Wu, Lingfei and Wang, Dashun and Evans, James A.},
	urldate = {2025-11-28},
	date = {2019-02},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Phase transitions and critical phenomena, Policy, Sociology},
}

@inproceedings{mitchell_model_2019,
	location = {New York, {NY}, {USA}},
	title = {Model Cards for Model Reporting},
	isbn = {978-1-4503-6125-5},
	url = {https://dl.acm.org/doi/10.1145/3287560.3287596},
	doi = {10.1145/3287560.3287596},
	series = {{FAT}* '19},
	abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
	pages = {220--229},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
	urldate = {2025-11-28},
	date = {2019-01-29},
}

@article{mokander_ethics-based_2021,
	title = {Ethics-Based Auditing of Automated Decision-Making Systems: Nature, Scope, and Limitations},
	volume = {27},
	issn = {1471-5546},
	url = {https://doi.org/10.1007/s11948-021-00319-4},
	doi = {10.1007/s11948-021-00319-4},
	shorttitle = {Ethics-Based Auditing of Automated Decision-Making Systems},
	abstract = {Important decisions that impact humans lives, livelihoods, and the natural environment are increasingly being automated. Delegating tasks to so-called automated decision-making systems ({ADMS}) can improve efficiency and enable new solutions. However, these benefits are coupled with ethical challenges. For example, {ADMS} may produce discriminatory outcomes, violate individual privacy, and undermine human self-determination. New governance mechanisms are thus needed that help organisations design and deploy {ADMS} in ways that are ethical, while enabling society to reap the full economic and social benefits of automation. In this article, we consider the feasibility and efficacy of ethics-based auditing ({EBA}) as a governance mechanism that allows organisations to validate claims made about their {ADMS}. Building on previous work, we define {EBA} as a structured process whereby an entity’s present or past behaviour is assessed for consistency with relevant principles or norms. We then offer three contributions to the existing literature. First, we provide a theoretical explanation of how {EBA} can contribute to good governance by promoting procedural regularity and transparency. Second, we propose seven criteria for how to design and implement {EBA} procedures successfully. Third, we identify and discuss the conceptual, technical, social, economic, organisational, and institutional constraints associated with {EBA}. We conclude that {EBA} should be considered an integral component of multifaced approaches to managing the ethical risks posed by {ADMS}.},
	pages = {44},
	number = {4},
	journaltitle = {Science and Engineering Ethics},
	shortjournal = {Sci Eng Ethics},
	author = {Mökander, Jakob and Morley, Jessica and Taddeo, Mariarosaria and Floridi, Luciano},
	urldate = {2025-11-28},
	date = {2021-07-06},
	langid = {english},
	keywords = {Artificial intelligence, Ethics, Governance, Auditing, Automated decision-making},
}

@misc{wu_large_2017,
	title = {Large Teams Have Developed Science and Technology; Small Teams Have Disrupted It},
	url = {http://arxiv.org/abs/1709.02445},
	doi = {10.48550/arXiv.1709.02445},
	abstract = {Teams dominate the production of high-impact science and technology. Analyzing teamwork from more than 50 million papers, patents, and software products, 1954-2014, we demonstrate across this period that larger teams developed recent, popular ideas, while small teams disrupted the system by drawing on older and less prevalent ideas. Attention to work from large teams came immediately, while advances by small teams succeeded further into the future. Differences between small and large teams magnify with impact - small teams have become known for disruptive work and large teams for developing work. Differences in topic and re- search design account for part of the relationship between team size and disruption, but most of the effect occurs within people, controlling for detailed subject and article type. Our findings suggest the importance of supporting both small and large teams for the sustainable vitality of science and technology.},
	number = {{arXiv}:1709.02445},
	publisher = {{arXiv}},
	author = {Wu, Lingfei and Wang, Dashun and Evans, James A.},
	urldate = {2025-11-28},
	date = {2017-09-11},
	eprinttype = {arxiv},
	eprint = {1709.02445 [physics]},
	keywords = {Computer Science - Digital Libraries, Computer Science - Social and Information Networks, Physics - Physics and Society},
}

@misc{floridi_translating_2019,
	location = {Rochester, {NY}},
	title = {Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical},
	url = {https://papers.ssrn.com/abstract=3835010},
	doi = {10.2139/ssrn.3835010},
	shorttitle = {Translating Principles into Practices of Digital Ethics},
	abstract = {Modern digital technologies—from web-based services to Artificial Intelligence ({AI}) solutions—increasingly affect the daily lives of billions of people. Such innovation brings huge opportunities, but also concerns about design, development, and deployment of digital technologies. This article identifies and discusses five clusters of risk in the international debate about digital ethics: ethics shopping; ethics bluewashing; ethics lobbying; ethics dumping; and ethics shirking.},
	number = {3835010},
	publisher = {Social Science Research Network},
	author = {Floridi, Luciano},
	urldate = {2025-11-27},
	date = {2019-05-23},
	langid = {english},
	keywords = {Society, Applied ethics, Artificial Intelligence, Bluewashing, Lobbying},
}

@misc{mazzini_system_2019,
	location = {Rochester, {NY}},
	title = {A System of Governance for Artificial Intelligence through the Lens of Emerging Intersections between {AI} and {EU} Law},
	url = {https://papers.ssrn.com/abstract=3369266},
	abstract = {The work provides an overview and a comment of the Communication on Artificial Intelligence ({AI}) adopted by the European Commission in April 2018. By offering a bird’s-eye view of those law and policy areas potentially relevant for or affected by {AI}, the {AI} Communication sets the stage for understanding how pervasively and extensively {AI} is likely to be mainstreamed in our economies and societies. Whether it is about safety of products, liability, consumer protection, personal data protection or the foundational values, principles and rights on which the European project is based on, {AI} is very rapidly cutting across domains. The work identifies and investigates some of the many intersections between {AI} and {EU} law. Two main “disrupting” trends emerge.According to the first trend, {AI} seems to exercise some pressure on existing regulatory frameworks, such as in the areas of product safety, liability and consumer protection.As regards product safety, the main concerns seem to revolve around the unpredictability risk of {AI}. While certain factual characteristics (possibly limitations) of {AI} as it functions today cannot and should not be denied, the policy debate on {AI} safety should focus on what potential risks brought about by {AI} (or rather by specific {AI} applications) can be considered as socially acceptable when weighed against potential benefits. Even though the challenges posed by {AI} may generate some pressure on the existing {EU} product safety frameworks, it seems that {EU} safety law as a broader normative field has at its disposal a varied set of regulatory tools and approaches that can be relevant sources of inspiration and reference for a discussion on the safety of {AI}-powered products.In the field of product liability, although one should note that the Product Liability Directive ({PLD}) is not necessarily the only tool that can be invoked by victims in case of risks and damages linked to {AI}-powered products, there seem to be elements suggesting that {AI} (in general or with regard to certain of its product specific applications) may put under stress the continued suitability of the technology neutral design of the {PLD} - or at least some provisions thereof - to the extent that the {PLD} is expected to apply, in its current form, to both “smart” and “non-smart” products.The protection of consumers in the context of profiling and targeting practices in the business-to-consumers transactions is an area where the General Data Protection Regulation ({GDPR}) is particularly relevant. To the extent {GDPR} rules effectively enhance the data subjects’ empowerment vis-a-vis traders and/or curtail the ability of traders to engage in manipulative and unfair practices, then there may be less need for a fine-tuning of dedicated consumer law instruments (such as the Unfair Commercial Practices Directive, the Consumer Rights Directive and the Directive on Unfair Terms in Consumer Contracts) in order to take account of the specificities of commercial transactions mediated by sophisticated algorithms. At the same time, consumer protection could be an interesting testing ground for the potential of {AI} to empower consumers and civil society in general: the very same tools, techniques and methods used by companies to pursue their commercial interests could also serve the purpose to re-balance the traditional asymmetry of information, power and knowledge impacting negatively on consumers.Contrary to what happens in the legal domains mentioned above, a different “disrupting” trend emerges in the field of the protection of personal data. Here, the several intersections between {AI} and the {GDPR} can essentially be framed in terms of the law disrupting certain technological uses and applications of {AI}. Due to the fact that {AI} uses and applications in the context of commercial transactions, and, more generally, of the algorithm-mediated economic, social and political life of individuals extensively rely on and process personal data, the {GDPR} emerges as a key piece of legislation in the space. While the data protection authorities and the courts will certainly specify and fine-tune its principles and provisions as appropriate, the {GDPR} presents itself as a robust framework poised to capture and effectively curb at least those uses and applications of {AI} that appear most egregious and intolerable in light of the degree of legal protection for individual rights and freedoms that is currently expected by citizens in our European society.The work argues that, even if each legal or policy area where {AI} surfaces is confronted with distinct normative questions that may not necessarily be relevant for other areas, a connecting tissue is needed. This should take the form of a system of {AI} governance or cabine de regie which should combine - on an ongoing basis - up-to-date scientific and technical knowledge, internal legal and policy expertise specific to each sector and the authority to impart policy direction and to arbitrate, across the board, between the societal opportunities and the societal concerns that underlie the composite interaction between {AI} and the law.},
	number = {3369266},
	publisher = {Social Science Research Network},
	author = {Mazzini, Gabriele},
	urldate = {2025-11-27},
	date = {2019-04-09},
	langid = {english},
	keywords = {{SSRN}, A System of Governance for Artificial Intelligence through the Lens of Emerging Intersections between {AI} and {EU} Law, Gabriele Mazzini},
}

@article{benkler_dont_2019,
	title = {Don’t let industry write the rules for {AI}},
	volume = {569},
	rights = {2019 Springer Nature Limited},
	url = {https://www.nature.com/articles/d41586-019-01413-1},
	doi = {10.1038/d41586-019-01413-1},
	abstract = {Technology companies are running a campaign to bend research and regulation for their benefit; society must fight back, says Yochai Benkler.},
	pages = {161--161},
	number = {7755},
	journaltitle = {Nature},
	author = {Benkler, Yochai},
	urldate = {2025-11-27},
	date = {2019-05-01},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: World View
Publisher: Nature Publishing Group
Subject\_term: Computer science, Society},
	keywords = {Computer science, Society},
}

@misc{manheim_artificial_2018,
	location = {Rochester, {NY}},
	title = {Artificial Intelligence: Risks to Privacy and Democracy},
	url = {https://papers.ssrn.com/abstract=3273016},
	shorttitle = {Artificial Intelligence},
	abstract = {A “Democracy Index” is published annually by the Economist.  For 2017, it reported that half of the world’s countries scored lower than the previous year.  This included the United States, which was demoted from “full democracy” to “flawed democracy.”  The principal factor was “ero-sion of confidence in government and public institutions.”  Interference by Russia and voter manipulation by Cambridge Analytica in the 2016 presi-dential election played a large part in that public disaffection.  Threats of these kinds will continue, fueled by growing deployment of artificial intelligence ({AI}) tools to manipulate the preconditions and levers of democracy.  Equally destructive is {AI}’s threat to decisional and informa-tional privacy.  {AI} is the engine behind Big Data Analytics and the Internet of Things.  While conferring some consumer benefit, their principal function at present is to capture personal information, create detailed behavioral profiles and sell us goods and agendas.  Privacy, anonymity and autonomy are the main casualties of {AI}’s ability to manipulate choices in economic and political decisions.The way forward requires greater attention to these risks at the nation-al level, and attendant regulation.  In its absence, technology giants, all of whom are heavily investing in and profiting from {AI}, will dominate not only the public discourse, but also the future of our core values and democratic institutions.},
	number = {3273016},
	publisher = {Social Science Research Network},
	author = {Manheim, Karl M. and {Lyric Kaplan}},
	urldate = {2025-11-27},
	date = {2018-10-25},
	langid = {english},
	keywords = {{SSRN}, Artificial Intelligence: Risks to Privacy and Democracy, Karl M. Manheim, Lyric Kaplan},
}

@misc{houser_it_2020,
	location = {Rochester, {NY}},
	title = {It Is Time to Move Beyond the ‘{AI} Race’ Narrative: Why Investment and International Cooperation Must Win the Day},
	url = {https://papers.ssrn.com/abstract=3582641},
	shorttitle = {It Is Time to Move Beyond the ‘{AI} Race’ Narrative},
	abstract = {The United States has entered into technology races before, often with great success, for example, the moon landing. Yet, most of these successes were accomplished with local knowledge, even if the impact was global.  Fifth Generation Cellular Wireless (5G), the Internet of Things ({IoT}) and Artificial Intelligence ({AI}) are simply incapable of being fully cultivated in the same local environment; thus, we must understand these technologies within the context of a global community. However, current U.S. policy and the absence of cooperation amongst countries are leaving the technology ecosystem to operate in a competitive, war-like environment. 5G, the Internet of Things, and Artificial Intelligence ({AI}) — as part of a highly connected digital community — demand we consider each within a critical infrastructure framework, focusing on its place in a global environment. To accomplish this, we must think in cooperative mindsets, with a focus of investment, frameworks, and communities of trust, which create and progress ideas for the betterment of mankind.  This paper seeks to explore how the current {AI} race paradigm must be discarded and the necessary changes needed to create a framework to advance the responsible development of {AI}. Focusing on the immediate and long term needs of the technology ecosystem, with an emphasis on cooperation in investment and standards, the paper will explain the importance of 5G and {AI} in the context of global communications; will then explain why coordinated investment and standards are necessary; will return the focus to ethical considerations with global communities as key stakeholders; and will finally set out brief series of recommendations for immediate and long term investment and guidelines.},
	number = {3582641},
	publisher = {Social Science Research Network},
	author = {Houser, Kimberly and Raymond, Anjanette},
	urldate = {2025-11-27},
	date = {2020-04-22},
	langid = {english},
	keywords = {Innovation, Big Data, 5G, {AI} Race, artificial intelligence, Fifth Generation Cellular Wireless, International Cooperation, Internet of Things, Surveillance},
}

@misc{gensler_deep_2020,
	location = {Rochester, {NY}},
	title = {Deep Learning and Financial Stability},
	url = {https://papers.ssrn.com/abstract=3723132},
	doi = {10.2139/ssrn.3723132},
	abstract = {The financial sector is entering a new era of rapidly advancing data analytics as deep learning models are adopted into its technology stack. A subset of Artificial Intelligence, deep learning represents a fundamental discontinuity from prior analytical techniques, providing previously unseen predictive powers enabling significant opportunities for efficiency, financial inclusion, and risk mitigation. Broad adoption of deep learning, though, may over time increase uniformity, interconnectedness, and regulatory gaps. This paper maps deep learning’s key characteristics across five possible transmission pathways exploring how, as it moves to a mature stage of broad adoption, it may lead to financial system fragility and economy-wide risks. Existing financial sector regulatory regimes - built in an earlier era of data analytics technology - are likely to fall short in addressing the systemic risks posed by broad adoption of deep learning in finance. The authors close by considering policy tools that might mitigate these systemic risks.},
	number = {3723132},
	publisher = {Social Science Research Network},
	author = {Gensler, Gary and Bailey, Lily},
	urldate = {2025-11-27},
	date = {2020-11-01},
	langid = {english},
	keywords = {Artificial Intelligence, Deep Learning, Financial Stability, Neural Networks, Systemic Risk},
}

@misc{danielsson_artificial_2021,
	location = {Rochester, {NY}},
	title = {Artificial Intelligence and Systemic Risk},
	url = {https://papers.ssrn.com/abstract=3410948},
	doi = {10.2139/ssrn.3410948},
	abstract = {Artificial intelligence ({AI}) is rapidly changing how the financial system is operated, taking over core functions for both cost savings and operational efficiency reasons. {AI} will assist both risk managers and the financial authorities. However, it can destabilize the financial system, creating new tail risks and amplifying existing ones due to procyclicality, unknown-unknowns, the need for trust, and optimization against the system.},
	number = {3410948},
	publisher = {Social Science Research Network},
	author = {Danielsson, Jon and Macrae, Robert and Uthemann, Andreas},
	urldate = {2025-11-27},
	date = {2021-05-28},
	langid = {english},
	keywords = {Artificial intelligence, systemic risk},
}

@article{stamatopoulos_derivative_2024,
	title = {Derivative Pricing using Quantum Signal Processing},
	volume = {8},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2307.14310},
	doi = {10.22331/q-2024-04-30-1322},
	abstract = {Pricing financial derivatives on quantum computers typically includes quantum arithmetic components which contribute heavily to the quantum resources required by the corresponding circuits. In this manuscript, we introduce a method based on Quantum Signal Processing ({QSP}) to encode financial derivative payoffs directly into quantum amplitudes, alleviating the quantum circuits from the burden of costly quantum arithmetic. Compared to current state-of-the-art approaches in the literature, we find that for derivative contracts of practical interest, the application of {QSP} significantly reduces the required resources across all metrics considered, most notably the total number of T-gates by \${\textbackslash}sim 16\$x and the number of logical qubits by \${\textbackslash}sim 4\$x. Additionally, we estimate that the logical clock rate needed for quantum advantage is also reduced by a factor of \${\textbackslash}sim 5\$x. Overall, we find that quantum advantage will require \$4.7\$k logical qubits, and quantum devices that can execute \$10{\textasciicircum}9\$ T-gates at a rate of \$45\${MHz}. While in this work we focus specifically on the payoff component of the derivative pricing process where the method we present is most readily applicable, similar techniques can be employed to further reduce the resources in other applications, such as state preparation.},
	pages = {1322},
	journaltitle = {Quantum},
	shortjournal = {Quantum},
	author = {Stamatopoulos, Nikitas and Zeng, William J.},
	urldate = {2025-11-27},
	date = {2024-04-30},
	eprinttype = {arxiv},
	eprint = {2307.14310 [quant-ph]},
	keywords = {Quantitative Finance - Computational Finance, Quantum Physics},
}

@misc{jin_energy_2025,
	title = {The Energy Cost of Reasoning: Analyzing Energy Usage in {LLMs} with Test-time Compute},
	url = {http://arxiv.org/abs/2505.14733},
	doi = {10.48550/arXiv.2505.14733},
	shorttitle = {The Energy Cost of Reasoning},
	abstract = {Scaling large language models ({LLMs}) has driven significant advancements, yet it faces diminishing returns and escalating energy demands. This work explores how test-time compute ({TTC}) can serve as an energy-efficient complement to conventional scaling strategies by allocating additional computational resources at inference time rather than during training. Specifically, we investigate whether employing {TTC} can achieve superior accuracy-energy trade-offs compared to simply increasing model size. Our empirical analysis reveals that {TTC} surpasses traditional model scaling in accuracy/energy efficiency, with notable gains in tasks demanding complex reasoning rather than mere factual recall. Further, we identify a critical interaction between {TTC} performance and output sequence length, demonstrating that strategically adjusting compute resources at inference time according to query complexity can substantially enhance efficiency. Our findings advocate for {TTC} as a promising direction, enabling more sustainable, accurate, and adaptable deployment of future language models.},
	number = {{arXiv}:2505.14733},
	publisher = {{arXiv}},
	author = {Jin, Yunho and Wei, Gu-Yeon and Brooks, David},
	urldate = {2025-11-27},
	date = {2025-11-09},
	eprinttype = {arxiv},
	eprint = {2505.14733 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{maliakel_investigating_2025,
	title = {Investigating Energy Efficiency and Performance Trade-offs in {LLM} Inference Across Tasks and {DVFS} Settings},
	url = {http://arxiv.org/abs/2501.08219},
	doi = {10.48550/arXiv.2501.08219},
	abstract = {Large language models ({LLMs}) have shown significant improvements in many natural language processing ({NLP}) tasks, accelerating their rapid adoption across many industries. These models are resource-intensive, requiring extensive computational resources both during training and inference, leading to increased energy consumption and negative environmental impact. As their adoption accelerates, the sustainability of {LLMs} has become a critical issue, necessitating strategies to optimize their runtime efficiency without compromising performance. Hence, it is imperative to identify the parameters that significantly influence the performance and energy efficiency of {LLMs}. To that end, in this work, we investigate the effect of important parameters on the performance and energy efficiency of {LLMs} during inference and examine their trade-offs. First, we analyze how different types of models with varying numbers of parameters and architectures perform on tasks like text generation, question answering, and summarization by benchmarking {LLMs} such as Falcon-7B, Mistral-7B-v0.1, T5-3B, {GPT}-2, {GPT}-J-6B, and {GPT}-Neo-2.7B. Second, we study input and output sequence characteristics such as sequence length concerning energy consumption, performance, and throughput. Finally, we explore the impact of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency Scaling ({DVFS}), on the models' latency and energy efficiency. Our extensive benchmarking and statistical analysis reveal many interesting findings, uncovering how specific optimizations can reduce energy consumption while maintaining throughput and accuracy. This study provides actionable insights for researchers and practitioners to design energy-efficient {LLM} inference systems.},
	number = {{arXiv}:2501.08219},
	publisher = {{arXiv}},
	author = {Maliakel, Paul Joe and Ilager, Shashikant and Brandic, Ivona},
	urldate = {2025-11-27},
	date = {2025-01-14},
	eprinttype = {arxiv},
	eprint = {2501.08219 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning},
}

@misc{patterson_carbon_2022,
	title = {The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink},
	url = {http://arxiv.org/abs/2204.05149},
	doi = {10.48550/arXiv.2204.05149},
	abstract = {Machine Learning ({ML}) workloads have rapidly grown in importance, but raised concerns about their carbon footprint. Four best practices can reduce {ML} training energy by up to 100x and {CO}2 emissions up to 1000x. By following best practices, overall {ML} energy use (across research, development, and production) held steady at {\textless}15\% of Google's total energy use for the past three years. If the whole {ML} field were to adopt best practices, total carbon emissions from training would reduce. Hence, we recommend that {ML} papers include emissions explicitly to foster competition on more than just model quality. Estimates of emissions in papers that omitted them have been off 100x-100,000x, so publishing emissions has the added benefit of ensuring accurate accounting. Given the importance of climate change, we must get the numbers right to make certain that we work on its biggest challenges.},
	number = {{arXiv}:2204.05149},
	publisher = {{arXiv}},
	author = {Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	urldate = {2025-11-27},
	date = {2022-04-11},
	eprinttype = {arxiv},
	eprint = {2204.05149 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - General Literature, Computer Science - Machine Learning},
}

@article{wang_option_2024,
	title = {Option pricing under stochastic volatility on a quantum computer},
	volume = {8},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2312.15871},
	doi = {10.22331/q-2024-10-23-1504},
	abstract = {We develop quantum algorithms for pricing Asian and barrier options under the Heston model, a popular stochastic volatility model, and estimate their costs, in terms of T-count, T-depth and number of logical qubits, on instances under typical market conditions. These algorithms are based on combining well-established numerical methods for stochastic differential equations and quantum amplitude estimation technique. In particular, we empirically show that, despite its simplicity, weak Euler method achieves the same level of accuracy as the better-known strong Euler method in this task. Furthermore, by eliminating the expensive procedure of preparing Gaussian states, the quantum algorithm based on weak Euler scheme achieves drastically better efficiency than the one based on strong Euler scheme. Our resource analysis suggests that option pricing under stochastic volatility is a promising application of quantum computers, and that our algorithms render the hardware requirement for reaching practical quantum advantage in financial applications less stringent than prior art.},
	pages = {1504},
	journaltitle = {Quantum},
	shortjournal = {Quantum},
	author = {Wang, Guoming and Kan, Angus},
	urldate = {2025-11-27},
	date = {2024-10-23},
	eprinttype = {arxiv},
	eprint = {2312.15871 [quant-ph]},
	keywords = {Quantum Physics},
}

@misc{jegham_how_2025,
	title = {How Hungry is {AI}? Benchmarking Energy, Water, and Carbon Footprint of {LLM} Inference},
	url = {http://arxiv.org/abs/2505.09598},
	doi = {10.48550/arXiv.2505.09598},
	shorttitle = {How Hungry is {AI}?},
	abstract = {As large language models ({LLMs}) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates {AI}'s environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of {LLM} inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public {API} performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis ({DEA}) to rank models by performance relative to environmental cost. Our results show that o3 and {DeepSeek}-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of {GPT}-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short {GPT}-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of {LLM} deployments, laying a foundation for future environmental accountability in {AI} development and sustainability standards.},
	number = {{arXiv}:2505.09598},
	publisher = {{arXiv}},
	author = {Jegham, Nidhal and Abdelatti, Marwen and Elmoubarki, Lassad and Hendawi, Abdeltawab},
	urldate = {2025-11-27},
	date = {2025-05-14},
	eprinttype = {arxiv},
	eprint = {2505.09598 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@article{dauner_energy_2025,
	title = {Energy costs of communicating with {AI}},
	volume = {10},
	issn = {2297-900X},
	url = {https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1572947/full},
	doi = {10.3389/fcomm.2025.1572947},
	abstract = {This study presents a comprehensive evaluation of the environmental cost of large language models ({LLMs}) by analyzing their performance, token usage, and {CO}2 equivalent emissions across 14 {LLMs} ranging from 7 to 72 billion parameters. Each {LLM} was tasked with answering 500 multiple-choice and 500 free-response questions from the {MMLU} benchmark, covering five diverse subjects. Emissions were measured using the Perun framework on an {NVIDIA} A100 {GPU} and converted through an emission factor of 480 {gCO}2/{kWh}. Our results reveal strong correlations between {LLM} size, reasoning behavior, token generation, and emissions. While larger and reasoning-enabled models achieve higher accuracy, up to 84.9\%, they also incur substantially higher emissions, driven largely by increased token output. Subject-level analysis further shows that symbolic and abstract domains such as Abstract Algebra consistently demand more computation and yield lower accuracy. These findings highlight the trade-offs between accuracy and sustainability, emphasizing the need for more efficient reasoning strategies in future {LLM} developments.},
	journaltitle = {Frontiers in Communication},
	shortjournal = {Front. Commun.},
	author = {Dauner, Maximilian and Socher, Gudrun},
	urldate = {2025-11-27},
	date = {2025-06-19},
	note = {Publisher: Frontiers},
	keywords = {sustainability, {CO}2 emission, {CO}2 equivalent, energy costs, large language model ({LLM})},
}

@article{stamatopoulos_option_2020,
	title = {Option Pricing using Quantum Computers},
	volume = {4},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/1905.02666},
	doi = {10.22331/q-2020-07-06-291},
	abstract = {We present a methodology to price options and portfolios of options on a gate-based quantum computer using amplitude estimation, an algorithm which provides a quadratic speedup compared to classical Monte Carlo methods. The options that we cover include vanilla options, multi-asset options and path-dependent options such as barrier options. We put an emphasis on the implementation of the quantum circuits required to build the input states and operators needed by amplitude estimation to price the different option types. Additionally, we show simulation results to highlight how the circuits that we implement price the different option contracts. Finally, we examine the performance of option pricing circuits on quantum hardware using the {IBM} Q Tokyo quantum device. We employ a simple, yet effective, error mitigation scheme that allows us to significantly reduce the errors arising from noisy two-qubit gates.},
	pages = {291},
	journaltitle = {Quantum},
	shortjournal = {Quantum},
	author = {Stamatopoulos, Nikitas and Egger, Daniel J. and Sun, Yue and Zoufal, Christa and Iten, Raban and Shen, Ning and Woerner, Stefan},
	urldate = {2025-11-27},
	date = {2020-07-06},
	eprinttype = {arxiv},
	eprint = {1905.02666 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{hird_policy_2005,
	title = {Policy Analysis for What? The Effectiveness of Nonpartisan Policy Research Organizations},
	volume = {33},
	rights = {Copyright Policy Studies Organization Feb 2005},
	issn = {0190292X},
	url = {https://www.proquest.com/docview/210554882/abstract/534F48A48FE4AA7PQ/1},
	doi = {10.1111/j.1541-0072.2005.00093.x},
	shorttitle = {Policy Analysis for What?},
	abstract = {For the past several decades, scholars have studied the role of policy research in decision making. Depending how "use" is measured, the results have indicated only modest or indirect impacts. This study takes another direction to studying knowledge utilization, comparing the influence of nonpartisan policy research organizations in 19 states on their principal clients: state legislators. Nonpartisan research agencies represent a particularly rigorous test since the literature on knowledge utilization questions whether nonpartisan policy analysis organizations ({NPROs}) can remain neutral and contribute meaningfully to policymaking. Further, this analysis is comparative across states, studies institutions rather than specific reports or policy domains, and examines multiple types of "use." Using the results of surveys of both agencies and legislators, the results indicate a strong association between the size and type of policy research organizations and legislators' assessments of use, even controlling for political and ideological factors. This suggests that policy analysis organizations-even those in a highly politicized environment-have a significant impact on policymaking, mainly by providing information and analysis to decision makers but also in influencing public policy outcomes. [{PUBLICATION} {ABSTRACT}]},
	pages = {83--105},
	number = {1},
	journaltitle = {Policy Studies Journal},
	author = {Hird, John A.},
	urldate = {2025-12-29},
	date = {2005-02},
	note = {Num Pages: 23
Place: Washington, United Kingdom
Publisher: Blackwell Publishing Ltd.},
	keywords = {Analysis, Congressional committees, Decision makers, Decision making, Information sources, Interest groups, Knowledge, Knowledge utilization, Legislators, Legislatures, Lobbyists, Organizational effectiveness, Organizations, Policy analysis, Policy making, Policy research, Politics, Public policy, Research, Research centers, Think tanks},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\3VP3FMXW\\Hird - 2005 - Policy Analysis for What The Effectiveness of Nonpartisan Policy Research Organizations.pdf:application/pdf},
}

@article{carlson_trends_2011,
	title = {Trends and Innovations in Public Policy Analysis},
	volume = {39},
	rights = {Copyright Policy Studies Organization Apr 2011},
	issn = {0190292X},
	url = {https://www.proquest.com/docview/859875293/abstract/D5C50406B94F4D59PQ/1},
	doi = {10.1111/j.1541-0072.2010.00389_2.x},
	abstract = {This essay identifies three notable advances that have influenced the field of public policy analysis in recent years: the move toward social experimentation, the use of meta-analysis and Monte Carlo simulation in benefit-cost analysis, and the rise of institutional actors that promote the practice and dissemination of high-quality policy analysis. In addition to describing each of these innovations, this essay discusses how each of these advances has affected the practice of public policy analysis. [{PUBLICATION} {ABSTRACT}]},
	pages = {13--26},
	issue = {S1},
	journaltitle = {Policy Studies Journal},
	author = {Carlson, Deven},
	urldate = {2025-12-29},
	date = {2011-04},
	note = {Num Pages: 14
Place: Washington, United Kingdom
Publisher: Blackwell Publishing Ltd.},
	keywords = {Analysis, Cost analysis, Cost benefit analysis, Dissemination, Education, Estimates, Experiments, Innovations, Intervention, Meta-analysis, Monte Carlo simulation, Policy analysis, Policy making, Public policy, Simulation, Social policy},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\DJN2XCFV\\Carlson - 2011 - Trends and Innovations in Public Policy Analysis.pdf:application/pdf},
}

@collection{weible_theories_2023,
	location = {New York London},
	edition = {Fifth edition},
	title = {Theories of the policy process},
	isbn = {978-1-032-31124-1 978-1-003-30820-1},
	abstract = {"Theories of the Policy Process provides a forum for the experts in policy process research to present the basic propositions, empirical evidence, latest updates, and the promising future research opportunities of each policy process theory. In this thoroughly revised fifth edition, each chapter has been updated to reflect recent empirical work, innovative theorizing, and a world facing challenges of historic proportions with climate change, social and political inequities, and pandemics, among recent events. Updated and revised chapters include Punctuated Equilibrium Theory, Multiple Streams Framework, Policy Feedback Theory, Advocacy Coalition Framework, Narrative Policy Framework, Institutional and Analysis and Development Framework, and Diffusion and Innovation. This fifth edition includes an entirely new chapter on the Ecology of Games Framework. New authors have been added to most chapters to diversify perspectives and make this latest edition the most internationalized yet. Across the chapters, revisions have clarified concepts and theoretical arguments, expanded and extended the theories' scope, summarized lessons learned and knowledge gained, and addressed the relevancy of policy process theories. Theories of the Policy Process has been, and remains, the quintessential gateway to the field of policy process research for students, scholars, and practitioners. It's ideal for those enrolled in policy process courses at the undergraduate and graduate levels, and those conducting research or undertaking practice in the subject"--},
	pagetotal = {1},
	publisher = {Routledge, Taylor \& Francis Group},
	editor = {Weible, Christopher M.},
	date = {2023},
}

@book{birkland_introduction_2020,
	location = {New York, {NY} London},
	edition = {Fifth edition},
	title = {An introduction to the policy process: theories, concepts, and models of public policy making},
	isbn = {978-1-138-49561-6 978-1-351-02394-8},
	shorttitle = {An introduction to the policy process},
	abstract = {Introducing the policy process -- Elements of the policy making system -- The historical and structural contexts of public policy making -- Official actors and their roles in public policy -- Unofficial actors and their roles in public policy -- Agenda setting, power, and interest groups -- Policies and policy types -- Decision making and policy analysis -- Policy design and policy tools -- Policy implementation, failure, and learning -- Science and theory in the study of public policy},
	pagetotal = {1},
	publisher = {Routledge, Taylor \& Francis Group},
	author = {Birkland, Thomas A.},
	date = {2020},
	doi = {10.4324/9781351023948},
	file = {Table of Contents PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\QV5LHKBP\\Birkland - 2020 - An introduction to the policy process theories, concepts, and models of public policy making.pdf:application/pdf},
}

@book{bardach_practical_2024,
	location = {London, United Kingdom},
	edition = {Seventh edition},
	title = {A practical guide for policy analysis: the eightfold path to more effective problem solving},
	isbn = {978-1-0718-8413-3},
	shorttitle = {A practical guide for policy analysis},
	abstract = {"This handbook serves as a guide to concepts and methods applied in the analysis of policy. Eugene Bardach developed the general approach and many of the specific suggestions over thirty-five years of teaching policy analysis workshops to first- and second-year graduate students at the Richard and Rhoda Goldman School of Public Policy, University of California, Berkeley. In the handbook's earliest incarnation, the ideas took form slowly and were conveyed to students in lectures. But because Bardach and his faculty colleagues systematically overloaded their students with work, the students would sometimes skip a lecture-and thus miss out on ideas that he regarded as essential. Bardach determined that if he were to create a handout for the students, at least he would be discharging his responsibility, and it would be up to the students to retrieve the ideas they missed. Over the years, as the handout grew, it was disseminated informally to colleagues at other universities and was posted on the website of the Electronic Hallway, based at the University of Washington. This book is the outgrowth of these previous compilations and the product of many years of experience. Eric M. Patashnik was first exposed to the Eightfold Path when he took the Introductory Policy Analysis course as a student at the Goldman School in the spring of 1988. As a professor, he has assigned earlier editions of Gene's book to hundreds of public policy students at {UCLA}, the University of Virginia, and Brown University. The presumed user is a beginning practitioner preparing to undertake a policy analysis, such as one of our master's students at Berkeley or Brown. But we have found this handbook useful at both ends of the spectrum-in teaching undergraduate Introduction to Public Policy courses as well as executive education groups. The handbook assumes a familiarity with basic economic concepts, including those having to do with market failures (including market imperfections). It is not meant to stand alone but should be used in conjunction with other sources, including some of the best textbooks in policy analysis, which are cited often to amplify points in this handbook: Behn and Vaupel (1982); Friedman (2002); {MacRae} and Whittington (1997); Morgan and Henrion (1990); Stokey and Zeckhauser (1978); and Weimer and Vining (2017). A book similar in spirit to this one, and that has many examples drawn from New Zealand and Australia, is Scott and Baehler (2010)." --},
	pagetotal = {205},
	publisher = {Sage},
	author = {Bardach, Eugene and Patashnik, Eric M.},
	date = {2024},
	file = {Table of Contents PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\QRJEDK68\\Bardach and Patashnik - 2024 - A practical guide for policy analysis the eightfold path to more effective problem solving.pdf:application/pdf},
}

@book{stone_policy_2012,
	location = {New York},
	edition = {3rd ed},
	title = {Policy paradox: the art of political decision making},
	isbn = {978-0-393-91272-2},
	shorttitle = {Policy paradox},
	pagetotal = {408},
	publisher = {W.W. Norton \& Co},
	author = {Stone, Deborah},
	date = {2012},
	keywords = {Economic aspects, Policy sciences, Political planning},
}

@book{althaus_australian_2023,
	location = {Abingdon, Oxon},
	edition = {[Seventh edition]},
	title = {The Australian policy handbook: a practical guide to the policymaking process},
	isbn = {978-1-003-35199-3},
	shorttitle = {The Australian policy handbook},
	abstract = {"The seventh edition of this classic handbook on the policy process is fully updated, featuring new material on policy making amid local and global disruption, the contestable nature of modern policy advice, commissioning and contracting, public engagement and policy success and failure. The Australian Policy Handbook shows how public policy permeates every aspect of our lives. It is the stuff of government, justifying taxes, driving legislation and shaping our social services. Public policy gives us roads, railways and airports, emergency services, justice, education and health services, defence, industry development and natural resource management. While politicians make the decisions, public servants provide analysis and support for those choices. This updated edition includes new visuals and introduces a series of case studies for the first time. These cases-covering family violence, behavioural economics, justice reinvestment, child protection and more-illustrate the personal and professional challenges of policymaking practice. Drawing on their extensive practical and academic experience, the authors outline the processes used in making public policy. They systematically explain the relationships between political decision makers, public service advisers, community participants and those charged with implementation. The Australian Policy Handbook remains the essential guide for students and practitioners of policy making in Australia"-- Provided by publisher},
	publisher = {Routledge},
	author = {Althaus, Catherine and Ball, Sarah and Bridgman, Peter and Davis, Glyn and Threlfall, David},
	date = {2023},
	note = {{OCLC}: 1346125469},
}

@book{barnes_modern_2023,
	location = {Cambridge},
	title = {Modern statutory interpretation: framework, principles and practice},
	isbn = {978-1-108-81602-1 978-1-108-89563-7},
	shorttitle = {Modern statutory interpretation},
	abstract = {An original, clear, coherent and research-based account of contemporary Australian statutory interpretation. It provides a comprehensive coverage of statutory interpretation law, legislative drafting, the parliamentary process, the modern history of interpretation, sources of doubt, and interpretation techniques},
	pagetotal = {1},
	publisher = {Cambridge University Press},
	author = {Barnes, Jeffrey and Dharmananda, Jacinta and Moran, Eamonn},
	date = {2023},
	doi = {10.1017/9781108895637},
}

@book{smith_writing_2023,
	location = {New York},
	edition = {Sixth edition},
	title = {Writing public policy: a practical guide to communicating in the policymaking process},
	isbn = {978-0-19-764349-5},
	shorttitle = {Writing public policy},
	pagetotal = {234},
	publisher = {Oxford University Press},
	author = {Smith, Catherine F.},
	date = {2023},
	file = {Table of Contents PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\J6JNPB3N\\Smith - 2023 - Writing public policy a practical guide to communicating in the policymaking process.pdf:application/pdf},
}

@book{pennock_cq_2023,
	location = {Thousand Oaks, California},
	edition = {Second edition},
	title = {The {CQ} Press writing guide for public policy},
	isbn = {978-1-0718-5828-8},
	abstract = {"The {CQ} Press Writing Guide for Public Policy", Second Edition, is loaded with rich real-world examples that help students master the process of translating insightful policy analysis into clear policy recommendations. Known for his conversational writing style, author Andrew Pennock offers step-by-step instructions on how to write for a variety of genres ... including policy memos, briefs, op-eds, press releases, written testimony, social media, and emails ... in a style that policy makers expect. Focusing on an audience-centered approach, the second edition of this award-winning book helps students learn how to create and organize an argument based on the unique needs and expectations of policy makers. The book also covers the nuts and bolts of how to write for a policy audience in specific genres, with special consideration of ethics and working with visual and technical material. The second edition features updated examples and a new chapter on writing for public speaking. Winner of the 2019 Textbook Excellence Award ("Texty") from the Textbook \& Academic Authors Association ({TAA})},
	pagetotal = {263},
	publisher = {{CQ} Press},
	author = {Pennock, Andrew},
	date = {2023},
}

@article{chigbu_science_2023,
	title = {The Science of Literature Reviews: Searching, Identifying, Selecting, and Synthesising},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2304-6775},
	url = {https://www.mdpi.com/2304-6775/11/1/2},
	doi = {10.3390/publications11010002},
	shorttitle = {The Science of Literature Reviews},
	abstract = {The ability to conduct an explicit and robust literature review by students, scholars or scientists is critical in producing excellent journal article...},
	number = {1},
	journaltitle = {Publications},
	author = {Chigbu, Uchendu Eugene and Atiku, Sulaiman Olusegun and Plessis, Cherley C. Du and Chigbu, Uchendu Eugene and Atiku, Sulaiman Olusegun and Plessis, Cherley C. Du},
	urldate = {2025-12-30},
	date = {2023-01-06},
	langid = {english},
	note = {Company: Multidisciplinary Digital Publishing Institute
Distributor: Multidisciplinary Digital Publishing Institute
Institution: Multidisciplinary Digital Publishing Institute
Label: Multidisciplinary Digital Publishing Institute
Publisher: publisher},
	keywords = {higher education, literature review, literature search, literature survey, postgraduate students, publication, research design, research method, research process, scientific paper writing},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\CNAX6BRS\\Chigbu et al. - 2023 - The Science of Literature Reviews Searching, Identifying, Selecting, and Synthesising.pdf:application/pdf},
}

@article{palmatier_review_2018,
	title = {Review articles: purpose, process, and structure},
	volume = {46},
	issn = {1552-7824},
	url = {https://doi.org/10.1007/s11747-017-0563-4},
	doi = {10.1007/s11747-017-0563-4},
	shorttitle = {Review articles},
	pages = {1--5},
	number = {1},
	journaltitle = {Journal of the Academy of Marketing Science},
	shortjournal = {J. of the Acad. Mark. Sci.},
	author = {Palmatier, Robert W. and Houston, Mark B. and Hulland, John},
	urldate = {2025-12-30},
	date = {2018-01-01},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\XYEP2DN4\\Palmatier et al. - 2018 - Review articles purpose, process, and structure.pdf:application/pdf},
}

@article{watson_how_2020,
	title = {How to undertake a literature search: a step-by-step guide},
	volume = {29},
	issn = {0966-0461},
	url = {https://www.magonlinelibrary.com/doi/abs/10.12968/bjon.2020.29.7.431},
	doi = {10.12968/bjon.2020.29.7.431},
	shorttitle = {How to undertake a literature search},
	abstract = {Undertaking a literature search can be a daunting prospect. Breaking the exercise down into smaller steps will make the process more manageable. This article suggests 10 steps that will help readers complete this task, from identifying key concepts to choosing databases for the search and saving the results and search strategy. It discusses each of the steps in a little more detail, with examples and suggestions on where to get help. This structured approach will help readers obtain a more focused set of results and, ultimately, save time and effort.},
	pages = {431--435},
	number = {7},
	journaltitle = {British Journal of Nursing},
	shortjournal = {Br J Nurs},
	author = {Watson, Mandy},
	urldate = {2025-12-30},
	date = {2020-04-09},
	note = {Publisher: Mark Allen Group},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\ERW27AGK\\Watson - 2020 - How to undertake a literature search a step-by-step guide.pdf:application/pdf},
}

@book{de_brun_searching_2014,
	location = {Chichester, West Sussex},
	edition = {2nd edition},
	title = {Searching skills toolkit: finding the evidence},
	isbn = {978-1-118-46309-3},
	series = {{EBM} toolkit series},
	shorttitle = {Searching skills toolkit},
	abstract = {This is a user-friendly, hands-on guide to literature searching, which is an essential skill for all involved in health care research and development, researchers, and students from all disciplines},
	publisher = {Wiley-Blackwell, {BMJ}/Books},
	author = {De Brún, Caroline and Pearce-Smith, Nicola},
	date = {2014},
}

@article{portman_how_2025,
	title = {How does social-media-based science communication affect young audiences? A scoping review of impact making},
	volume = {24},
	rights = {https://jcom.sissa.it/site/about-jcom},
	issn = {1824-2049},
	url = {https://jcom.sissa.it/article/pubid/JCOM_2405_2025_V02/},
	doi = {10.22323/145420250918092124},
	shorttitle = {How does social-media-based science communication affect young audiences?},
	abstract = {While social media has been praised for youth engagement with science, evidence of its impacts remains fragmented. This scoping review reports on the impacts of social-media-based science communication on young audiences. A {PRISMA}-guided database search yielded 2,257 articles, which were screened to include only},
	pages = {V02},
	number = {5},
	journaltitle = {Journal of Science Communication},
	shortjournal = {{JCOM}},
	author = {Portman, Jordana and Miara Ms, Victoria Yael and Baram-Tsabari, Ayelet},
	urldate = {2026-01-02},
	date = {2025-10-13},
	note = {Publisher: {SISSA} Medialab srl},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\IWTDD97Y\\Portman et al. - 2025 - How does social-media-based science communication affect young audiences A scoping review of impact.pdf:application/pdf},
}

@article{roland_social_2018,
	title = {Social Media, Health Policy, and Knowledge Translation},
	volume = {15},
	issn = {1546-1440},
	url = {https://www.sciencedirect.com/science/article/pii/S1546144017311249},
	doi = {10.1016/j.jacr.2017.09.009},
	series = {Social Media and Medicine},
	abstract = {Social media has been cited as a methodology for reducing the knowledge translation gap, creating communities of practice, and reducing traditional hierarchical divisions. Social movements have also embraced social media as a means of spreading their aims and reaching wide audiences. However, its impact on health policy is seldom considered. The author examines the complexity of clinicians’ use of social media to influence policy and how policy and government groups may use social media to help their own objectives.},
	pages = {149--152},
	number = {1},
	journaltitle = {Journal of the American College of Radiology},
	shortjournal = {Journal of the American College of Radiology},
	author = {Roland, Damian},
	urldate = {2026-01-02},
	date = {2018-01-01},
	keywords = {community of practice, health policy, knowledge translation, Social media},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\NASBDXE4\\Roland - 2018 - Social Media, Health Policy, and Knowledge Translation.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\YDC32XXT\\S1546144017311249.html:text/html},
}

@article{graham_lost_2006,
	title = {Lost in knowledge translation: Time for a map?},
	volume = {26},
	rights = {Copyright © 2006 Wiley Periodicals, Inc.},
	issn = {1554-558X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/chp.47},
	doi = {10.1002/chp.47},
	shorttitle = {Lost in knowledge translation},
	abstract = {There is confusion and misunderstanding about the concepts of knowledge translation, knowledge transfer, knowledge exchange, research utilization, implementation, diffusion, and dissemination. We review the terms and definitions used to describe the concept of moving knowledge into action. We also offer a conceptual framework for thinking about the process and integrate the roles of knowledge creation and knowledge application. The implications of knowledge translation for continuing education in the health professions include the need to base continuing education on the best available knowledge, the use of educational and other transfer strategies that are known to be effective, and the value of learning about planned-action theories to be better able to understand and influence change in practice settings.},
	pages = {13--24},
	number = {1},
	journaltitle = {Journal of Continuing Education in the Health Professions},
	author = {Graham, Ian D. and Logan, Jo and Harrison, Margaret B. and Straus, Sharon E. and Tetroe, Jacqueline and Caswell, Wenda and Robinson, Nicole},
	urldate = {2026-01-02},
	date = {2006},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/chp.47},
	keywords = {continuing education, continuing professional development, knowledge exchange, knowledge transfer, knowledge translation, research utilization},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\QH6F4CLP\\Graham et al. - 2006 - Lost in knowledge translation Time for a map.pdf:application/pdf;Snapshot:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\ZNNQS69I\\chp.html:text/html},
}

@incollection{wagner_knowledge_2015,
	title = {Knowledge Management},
	rights = {Copyright © 2014 John Wiley \& Sons, Ltd. All rights reserved.},
	isbn = {978-1-118-78531-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118785317.weom060128},
	abstract = {Knowledge is arguably the most important asset of firms and a central tenet of the knowledge-based view that regard firms as systems of knowledge production and application. In this context, two forms of knowledge – explicit and tacit knowledge – gain significant interest in literature and are discussed with respect to diverse knowledge management processes such as knowledge creation or knowledge transfer. Knowledge management deals with systematic processes to identify, acquire, create, share, apply, and maintain knowledge, and is critical for firms to fulfill their organizational objectives.},
	pages = {1--3},
	booktitle = {Wiley Encyclopedia of Management},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Wagner, Heinz-Theo and Moos, Bernhard},
	urldate = {2026-01-02},
	date = {2015},
	langid = {english},
	doi = {10.1002/9781118785317.weom060128},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118785317.weom060128},
	keywords = {explicit knowledge, knowledge diffusion, knowledge services, knowledge sharing, knowledge spillovers, knowledge transfer, knowledge-based view, tacit knowledge},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\GNBTVTB3\\Wagner and Moos - 2015 - Knowledge Management.pdf:application/pdf;Snapshot:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\LC8GTF3G\\9781118785317.html:text/html},
}

@article{klar_using_2020,
	title = {Using social media to promote academic research: Identifying the benefits of twitter for sharing academic work},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0229446},
	doi = {10.1371/journal.pone.0229446},
	shorttitle = {Using social media to promote academic research},
	abstract = {To disseminate research, scholars once relied on university media services or journal press releases, but today any academic can turn to Twitter to share their published work with a broader audience. The possibility that scholars can push their research out, rather than hope that it is pulled in, holds the potential for scholars to draw wide attention to their research. In this manuscript, we examine whether there are systematic differences in the types of scholars who most benefit from this push model. Specifically, we investigate the extent to which there are gender differences in the dissemination of research via Twitter. We carry out our analyses by tracking tweet patterns for articles published in six journals across two fields (political science and communication), and we pair this Twitter data with demographic and educational data about the authors of the published articles, as well as article citation rates. We find considerable evidence that, overall, article citations are positively correlated with tweets about the article, and we find little evidence to suggest that author gender affects the transmission of research in this new media.},
	pages = {e0229446},
	number = {4},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Klar, Samara and Krupnikov, Yanna and Ryan, John Barry and Searles, Kathleen and Shmargad, Yotam},
	urldate = {2026-01-02},
	date = {2020-04-06},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Bibliometrics, Citation analysis, Communications, Political science, Scientific publishing, Social media, Social networks, Twitter},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\J5LKDQGH\\Klar et al. - 2020 - Using social media to promote academic research Identifying the benefits of twitter for sharing aca.pdf:application/pdf},
}

@incollection{zickar_using_2020,
	title = {Using Social Media to Promote Academic Functioning},
	isbn = {978-3-030-10576-1},
	url = {https://link.springer.com/rwe/10.1007/978-3-030-10576-1_213},
	abstract = {'Using Social Media to Promote Academic Functioning' published in 'Encyclopedia of Education and Information Technologies'},
	pages = {1750--1752},
	booktitle = {Encyclopedia of Education and Information Technologies},
	publisher = {Springer, Cham},
	author = {Zickar, Michael J.},
	urldate = {2026-01-02},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-10576-1_213},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\MNPUHBNM\\Zickar - 2020 - Using Social Media to Promote Academic Functioning.pdf:application/pdf},
}

@article{monteiro-krebs_trespassing_2021,
	title = {Trespassing the gates of research: identifying algorithmic mechanisms that can cause distortions and biases in academic social media},
	volume = {46},
	issn = {1468-4527},
	url = {https://doi.org/10.1108/OIR-01-2021-0042},
	doi = {10.1108/OIR-01-2021-0042},
	shorttitle = {Trespassing the gates of research},
	abstract = {The use of recommender systems is increasing on academic social media ({ASM}). However, distinguishing the elements that may be influenced and/or exert influence over content that is read and disseminated by researchers is difficult due to the opacity of the algorithms that filter information on {ASM}. In this article, the purpose of this paper is to investigate how algorithmic mediation through recommender systems in {ResearchGate} may uphold biases in scholarly communication.The authors used a multi-method walkthrough approach including a patent analysis, an interface analysis and an inspection of the web page code.The findings reveal how audience influences on the recommendations and demonstrate in practice the mutual shaping of the different elements interplaying within the platform (artefact, practices and arrangements). The authors show evidence of the mechanisms of selection, prioritization, datafication and profiling. The authors also substantiate how the algorithm reinforces the reputation of eminent researchers (a phenomenon called the Matthew effect). As part of defining a future agenda, we discuss the need for serendipity and algorithmic transparency.Algorithms change constantly and are protected by commercial secrecy. Hence, this study was limited to the information that was accessible within a particular period. At the time of publication, the platform, its logic and its effects on the interface may have changed. Future studies might investigate other {ASM} using the same approach to distinguish potential patterns among platforms.Contributes to reflect on algorithmic mediation and biases in scholarly communication potentially afforded by recommender algorithms. To the best of our knowledge, this is the first empirical study on automated mediation and biases in {ASM}.},
	pages = {993--1013},
	number = {5},
	journaltitle = {Online Information Review},
	shortjournal = {Online Information Review},
	author = {Monteiro-Krebs, Luciana and Zaman, Bieke and Caregnato, Sonia Elisa and Geerts, David and Grassi-Filho, Vicente and Htun, Nyi-Nyi},
	urldate = {2026-01-02},
	date = {2021-12-21},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\92JR8PHN\\Monteiro-Krebs et al. - 2021 - Trespassing the gates of research identifying algorithmic mechanisms that can cause distortions and.pdf:application/pdf;Snapshot:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\EBCQRALS\\OIR-01-2021-0042.html:text/html},
}

@article{brownson_researchers_2006,
	title = {Researchers and Policymakers: Travelers in Parallel Universes},
	volume = {30},
	issn = {0749-3797},
	url = {https://www.sciencedirect.com/science/article/pii/S074937970500396X},
	doi = {10.1016/j.amepre.2005.10.004},
	shorttitle = {Researchers and Policymakers},
	abstract = {Public policy, in the form of laws, guidelines, and regulations, has a profound effect on our daily lives and health status. Reasons for a lack of consistent and systematic translation of public health research into public policy is examined, including differences in decision-making processes, poor timing, ambiguous findings, the need to balance objectivity and advocacy, personal demands of the process, information overload, lack of relevant data, and the mismatch of randomized thinking with nonrandom problems. Next, several actions are suggested that should help bridge the chasm between science and policy, such as greater involvement in the process, better understanding of political decision making, building of effective teams, and development of political champions. Scientists are obligated not only to discover new knowledge but also to ensure that discoveries are applied to improve health.},
	pages = {164--172},
	number = {2},
	journaltitle = {American Journal of Preventive Medicine},
	shortjournal = {American Journal of Preventive Medicine},
	author = {Brownson, Ross C. and Royer, Charles and Ewing, Reid and {McBride}, Timothy D.},
	urldate = {2026-01-02},
	date = {2006-02-01},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\QM7A5VU8\\Brownson et al. - 2006 - Researchers and Policymakers Travelers in Parallel Universes.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\Q2LBJ6KE\\S074937970500396X.html:text/html},
}

@article{metzler_social_2024,
	title = {Social Drivers and Algorithmic Mechanisms on Digital Media},
	volume = {19},
	issn = {1745-6916},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11373151/},
	doi = {10.1177/17456916231185057},
	abstract = {On digital media, algorithms that process data and recommend content have become ubiquitous. Their fast and barely regulated adoption has raised concerns about their role in well-being both at the individual and collective levels. Algorithmic mechanisms on digital media are powered by social drivers, creating a feedback loop that complicates research to disentangle the role of algorithms and already existing social phenomena. Our brief overview of the current evidence on how algorithms affect well-being, misinformation, and polarization suggests that the role of algorithms in these phenomena is far from straightforward and that substantial further empirical research is needed. Existing evidence suggests that algorithms mostly reinforce existing social drivers, a finding that stresses the importance of reflecting on algorithms in the larger societal context that encompasses individualism, populist politics, and climate change. We present concrete ideas and research questions to improve algorithms on digital platforms and to investigate their role in current problems and potential solutions. Finally, we discuss how the current shift from social media to more algorithmically curated media brings both risks and opportunities if algorithms are designed for individual and societal flourishing rather than short-term profit.},
	pages = {735--748},
	number = {5},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Metzler, Hannah and Garcia, David},
	urldate = {2026-01-02},
	date = {2024-09},
	pmid = {37466493},
	pmcid = {PMC11373151},
	file = {Full Text:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\UFK6NN8A\\Metzler and Garcia - 2024 - Social Drivers and Algorithmic Mechanisms on Digital Media.pdf:application/pdf},
}

@article{stone_chasing_2017,
	title = {Chasing after a Century of Punctuation},
	volume = {118},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050917323475},
	doi = {10.1016/j.procs.2017.11.144},
	series = {Data Analytics Summit {II}; Structuring the {UNSTRUCTURED}: The Missing Element of Analytics, 14-16 December 2015, Harrisburg, {USA}},
	abstract = {Punctuation in writing is a recent and Western invention. This study sought to determine whether using punctuation has been changing alongside changes in formal written communication in the last 11 decades. Through an analysis of 66 Introduction to Psychology textbooks published between the years 1895 and 2000, we determined that using question marks, colons, and semi-colons have increased over time, but that using exclamation points and commas have not. A post-hoc analysis of word frequencies in early and late textbook questions suggests that at least some differences may be related authors’ greater effort in recent decades to engage the reader.},
	pages = {15--21},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Stone, Alyson and Ford, W. Randolph},
	urldate = {2026-01-02},
	date = {2017-01-01},
	keywords = {Change, Electronic Health Records, Electronic Patient Records, Learning, Technology},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\BDXHXFMI\\Stone and Ford - 2017 - Chasing after a Century of Punctuation.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\X5NCYZCH\\S1877050917323475.html:text/html},
}

@article{page_prisma_2021,
	title = {{PRISMA} 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews},
	volume = {372},
	rights = {© Author(s) (or their employer(s)) 2019. Re-use permitted under {CC}                 {BY}. No commercial re-use. See rights and permissions. Published by                 {BMJ}.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution ({CC} {BY} 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/372/bmj.n160},
	doi = {10.1136/bmj.n160},
	shorttitle = {{PRISMA} 2020 explanation and elaboration},
	abstract = {{\textless}p{\textgreater}The methods and results of systematic reviews should be reported in sufficient detail to allow users to assess the trustworthiness and applicability of the review findings. The Preferred Reporting Items for Systematic reviews and Meta-Analyses ({PRISMA}) statement was developed to facilitate transparent and complete reporting of systematic reviews and has been updated (to {PRISMA} 2020) to reflect recent advances in systematic review methodology and terminology. Here, we present the explanation and elaboration paper for {PRISMA} 2020, where we explain why reporting of each item is recommended, present bullet points that detail the reporting recommendations, and present examples from published reviews. We hope that changes to the content and structure of {PRISMA} 2020 will facilitate uptake of the guideline and lead to more transparent, complete, and accurate reporting of systematic reviews.{\textless}/p{\textgreater}},
	pages = {n160},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Page, Matthew J. and Moher, David and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and {McDonald}, Steve and {McGuinness}, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and {McKenzie}, Joanne E.},
	urldate = {2026-01-02},
	date = {2021-03-29},
	langid = {english},
	pmid = {33781993},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\V6MNWIW3\\Page et al. - 2021 - PRISMA 2020 explanation and elaboration updated guidance and exemplars for reporting systematic rev.pdf:application/pdf},
}

@book{rossi_evaluation_1999,
	location = {Thousand Oaks, Calif},
	edition = {6th ed},
	title = {Evaluation : a systematic approach},
	isbn = {978-0-7619-0893-7},
	url = {https://research.ebsco.com/linkprocessor/plink?id=882b2cf1-c74d-3056-bf51-8138a623ffa8},
	shorttitle = {Evaluation},
	publisher = {{SAGE} Publications, Inc},
	author = {Rossi, Peter H. and Lipsey, Mark W. and Freeman, Howard E.},
	urldate = {2026-01-02},
	date = {1999-01-01},
	keywords = {Evaluation research (Social action programs), {POLITICAL} {SCIENCE} / Public Policy / Social Services \& Welfare, Social planning, {SOCIAL} {SCIENCE} / Human Services},
}

@article{hanck_joshua_2011,
	title = {Joshua D. Angrist and Jorn-Steffen Pischke (2009): Mostly Harmless Econometrics: An Empiricist's Companion},
	volume = {52},
	rights = {Springer-Verlag 2011},
	issn = {09325026},
	url = {https://www.proquest.com/docview/862921056/citation/C14B97D41599441BPQ/1},
	doi = {10.1007/s00362-009-0284-y},
	shorttitle = {Joshua D. Angrist and Jorn-Steffen Pischke (2009)},
	pages = {503--504},
	number = {2},
	journaltitle = {Statistical Papers},
	author = {Hanck, Christoph},
	urldate = {2026-01-02},
	date = {2011-05},
	note = {Num Pages: 503-504
Place: Heidelberg, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Books, Econometrics, Labor economics},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\V8RPJPTS\\Hanck - 2011 - Joshua D. Angrist and Jorn-Steffen Pischke (2009) Mostly Harmless Econometrics An Empiricist's Com.pdf:application/pdf},
}

@article{ram_git_2013,
	title = {Git can facilitate greater reproducibility and increased transparency in science},
	volume = {8},
	issn = {1751-0473},
	url = {https://doi.org/10.1186/1751-0473-8-7},
	doi = {10.1186/1751-0473-8-7},
	abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
	pages = {7},
	number = {1},
	journaltitle = {Source Code for Biology and Medicine},
	shortjournal = {Source Code Biol Med},
	author = {Ram, Karthik},
	urldate = {2026-01-02},
	date = {2013-02-28},
	langid = {english},
	keywords = {Open science, Reproducible research, Version control},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\CVK5IV9Y\\Ram - 2013 - Git can facilitate greater reproducibility and increased transparency in science.pdf:application/pdf},
}

@article{knauff_efficiency_2014,
	title = {An Efficiency Comparison of Document Preparation Systems Used in Academic Research and Development},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0115069},
	doi = {10.1371/journal.pone.0115069},
	abstract = {The choice of an efficient document preparation system is an important decision for any academic researcher. To assist the research community, we report a software usability study in which 40 researchers across different disciplines prepared scholarly texts with either Microsoft Word or {LaTeX}. The probe texts included simple continuous text, text with tables and subheadings, and complex text with several mathematical equations. We show that {LaTeX} users were slower than Word users, wrote less text in the same amount of time, and produced more typesetting, orthographical, grammatical, and formatting errors. On most measures, expert {LaTeX} users performed even worse than novice Word users. {LaTeX} users, however, more often report enjoying using their respective software. We conclude that even experienced {LaTeX} users may suffer a loss in productivity when {LaTeX} is used, relative to other document preparation systems. Individuals, institutions, and journals should carefully consider the ramifications of this finding when choosing document preparation strategies, or requiring them of authors.},
	pages = {e115069},
	number = {12},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Knauff, Markus and Nejasmic, Jelica},
	urldate = {2026-01-02},
	date = {2014-12-19},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Computer and information sciences, Computer engineering, Computer software, Ergonomics, Mathematical physics, Questionnaires, Scientists, Social psychology},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\44AFXM3B\\Knauff and Nejasmic - 2014 - An Efficiency Comparison of Document Preparation Systems Used in Academic Research and Development.pdf:application/pdf},
}

@article{peng_reproducible_2011,
	title = {Reproducible Research in Computational Science},
	volume = {334},
	issn = {0036-8075},
	url = {https://www.jstor.org/stable/41352177},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reprodudbility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	pages = {1226--1227},
	number = {6060},
	journaltitle = {Science},
	author = {Peng, Roger D.},
	urldate = {2026-01-02},
	date = {2011},
	note = {Publisher: American Association for the Advancement of Science},
	file = {JSTOR Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\4S4KVTUV\\Peng - 2011 - Reproducible Research in Computational Science.pdf:application/pdf},
}

@article{wilson_good_2017,
	title = {Good enough practices in scientific computing},
	volume = {13},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510},
	doi = {10.1371/journal.pcbi.1005510},
	abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
	pages = {e1005510},
	number = {6},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
	urldate = {2026-01-02},
	date = {2017-06-22},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Computer software, Control systems, Data management, Metadata, Programming languages, Reproducibility, Software tools, Source code},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\NMHQEAU8\\Wilson et al. - 2017 - Good enough practices in scientific computing.pdf:application/pdf},
}

@article{sandve_ten_2013,
	title = {Ten Simple Rules for Reproducible Computational Research},
	volume = {9},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	pages = {e1003285},
	number = {10},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	urldate = {2026-01-02},
	date = {2013-10-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Archives, Computer and information sciences, Computer applications, Genome analysis, Habits, Replication studies, Reproducibility, Source code},
	file = {Full Text PDF:C\:\\Users\\Zac Kienzle\\Zotero\\storage\\QJ8ZREZZ\\Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Research.pdf:application/pdf},
}
